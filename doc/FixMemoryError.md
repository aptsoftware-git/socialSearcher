# URGENT: Switch to gemma3:1b Model

## üö® Problem Identified

**Error:** `500 Internal Server Error: model requires more system memory (12.2 GiB) than is available (6.7 GiB)`

**Cause:** `gpt-oss:20b` requires 12.2 GiB RAM, but you only have 6.7 GiB

---

## ‚úÖ Solution: Use gemma3:1b (Already Installed!)

You have `gemma3:1b` installed - it only needs 1-2 GiB RAM!

---

## üîß Quick Fix Steps (5 minutes)

### 1. Test gemma3:1b Works

```cmd
ollama run gemma3:1b "Classify this event: Large protest in Mumbai"
```

**Expected:** Should return a response about protest event
**If it works:** Great! Continue to step 2

### 2. Update .env File

```cmd
cd c:\Anu\APT\apt\defender\scraping\code

# If .env doesn't exist yet, create it
copy .env.example .env

# Edit .env file
notepad .env
```

**Change this line:**
```
FROM: OLLAMA_MODEL=gpt-oss:20b
TO:   OLLAMA_MODEL=gemma3:1b
```

**Save and close Notepad**

### 3. If Backend is Running, Restart It

If you have the backend running:
```cmd
# Press CTRL+C in the terminal running uvicorn
# Then start it again:
uvicorn app.main:app --reload
```

### 4. Test the API

```cmd
curl http://localhost:8000/api/v1/ollama/status
```

**Expected Response:**
```json
{
  "status": "connected",
  "model": "gemma3:1b",
  "message": "Ollama is running and accessible"
}
```

### 5. Test Generation

```cmd
curl http://localhost:8000/api/v1/test/ollama
```

**Expected:** Should return a response without 500 error!

---

## üìä Model Comparison

| Model | Your Current | Recommended |
|-------|-------------|-------------|
| **Name** | gpt-oss:20b | gemma3:1b |
| **Size** | 13 GB | 815 MB |
| **RAM Required** | 12.2 GiB ‚ùå | 1-2 GiB ‚úÖ |
| **Speed** | Slow | Fast ‚ö°‚ö°‚ö° |
| **Will it work?** | ‚ùå NO | ‚úÖ YES |

---

## Why gemma3:1b is Perfect for Your Project

1. ‚úÖ **Fits in your 6.7 GiB RAM** - Only uses ~2 GiB
2. ‚úÖ **Already installed** - No download needed
3. ‚úÖ **Fast responses** - Good for web scraping
4. ‚úÖ **Good at structured output** - Perfect for JSON extraction
5. ‚úÖ **Designed for instructions** - Follows extraction prompts well

---

## Performance Expectations

### With gemma3:1b:
- ‚úÖ Speed: 1-2 seconds per article
- ‚úÖ Accuracy: 75-85% for event extraction
- ‚úÖ JSON Success: 90%+
- ‚úÖ System Impact: Low (leaves RAM for browser)

### With gpt-oss:20b (if you had 12+ GiB RAM):
- ‚ö° Speed: 5-10 seconds per article
- ‚ö° Accuracy: 90-95%
- ‚ö° System Impact: High (uses most RAM)

**Verdict:** gemma3:1b is actually BETTER for your use case - faster and fits your system!

---

## Alternative Options (If Needed)

If you want better accuracy than gemma3:1b:

### Option 1: phi3:mini (Good Balance)
```cmd
ollama pull phi3:mini
# Update .env: OLLAMA_MODEL=phi3:mini
```
- Size: 2.3 GB
- RAM: 3-4 GiB
- Accuracy: Better than gemma3:1b

### Option 2: llama3.2:3b (Best That Fits)
```cmd
ollama pull llama3.2:3b
# Update .env: OLLAMA_MODEL=llama3.2:3b
```
- Size: 2.0 GB
- RAM: 3-4 GiB
- Accuracy: Excellent
- Huge context window (128K tokens)

---

## Don't Want gpt-oss:20b Anymore?

If you want to free up the 13 GB disk space:

```cmd
ollama rm gpt-oss:20b
```

This will delete the model (you can always pull it again later if you upgrade RAM).

---

## Summary of Changes Made

I've updated these files to use gemma3:1b:

1. ‚úÖ `.env.example` - Changed default model to gemma3:1b
2. ‚úÖ `doc/ImplementationPlan.md` - Updated model recommendations
3. ‚úÖ `doc/Increment1_SetupGuide.md` - Updated setup instructions
4. ‚úÖ `doc/ModelRecommendations.md` - Created detailed comparison (NEW)
5. ‚úÖ `doc/FixMemoryError.md` - This quick action guide (NEW)

---

## Next Steps

1. ‚úÖ **Update .env** to use `OLLAMA_MODEL=gemma3:1b`
2. ‚úÖ **Test** with `curl http://localhost:8000/api/v1/test/ollama`
3. ‚úÖ **Continue** with Increment 1 setup from `doc/Increment1_Checklist.md`

---

**Need Help?**

If gemma3:1b doesn't give good enough results:
1. Try `phi3:mini` (slightly better accuracy)
2. Try `llama3.2:3b` (best that fits your RAM)
3. Consider upgrading RAM to 12+ GiB to use larger models

But start with gemma3:1b - it should work great for event extraction!

---

**Status:** Ready to test! üöÄ

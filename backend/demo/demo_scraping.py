"""
Practical demo of web scraping with configured sources.
"""

import sys
import asyncio
from pathlib import Path

# Add backend directory to path
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from app.services.scraper_manager import ScraperManager
from app.services.config_manager import config_manager


async def demo_scraping():
    """Demonstrate scraping from configured sources."""
    print("=" * 70)
    print("WEB SCRAPING DEMO")
    print("=" * 70)
    
    # Load sources
    sources = config_manager.load_sources()
    enabled_sources = [s for s in sources if s.enabled]
    
    print(f"\nüìã Configured Sources: {len(sources)}")
    print(f"   Enabled: {len(enabled_sources)}")
    print(f"   Disabled: {len(sources) - len(enabled_sources)}")
    
    print(f"\nüì∞ Enabled Sources:")
    for source in enabled_sources:
        print(f"   ‚Ä¢ {source.name}")
        print(f"     Base URL: {source.base_url}")
        print(f"     Rate Limit: {source.rate_limit}s")
        print(f"     Search Template: {'‚úì' if source.search_url_template else '‚úó'}")
    
    # Initialize scraper
    scraper = ScraperManager(timeout=30.0, max_retries=3)
    
    print(f"\nüîç Scraper Configuration:")
    print(f"   Timeout: {scraper.timeout}s")
    print(f"   Max Retries: {scraper.max_retries}")
    print(f"   Follow Redirects: {scraper.follow_redirects}")
    
    # Demo: Scrape a simple test article
    print(f"\n" + "=" * 70)
    print("DEMO: Scraping Test Article")
    print("=" * 70)
    
    # Use httpbin.org for reliable testing
    test_url = "https://httpbin.org/html"
    print(f"\nTest URL: {test_url}")
    
    try:
        html = await scraper.fetch_url(test_url, rate_limit=0.5)
        
        if html:
            print(f"‚úÖ Successfully fetched content")
            print(f"   Content length: {len(html)} characters")
            
            # Extract content
            extracted = scraper.content_extractor.extract_generic(html)
            print(f"\nüìÑ Extracted Content:")
            if extracted.get('title'):
                print(f"   Title: {extracted['title'][:60]}...")
            if extracted.get('content'):
                print(f"   Content: {len(extracted['content'])} chars")
                print(f"   Preview: {extracted['content'][:100]}...")
        else:
            print(f"‚ùå Failed to fetch content")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Show example of how to scrape from configured sources
    print(f"\n" + "=" * 70)
    print("EXAMPLE: How to Scrape from Configured Sources")
    print("=" * 70)
    
    print(f"""
To scrape articles from configured sources, use:

```python
# Scrape from all enabled sources
query = "protest in Mumbai"
articles = await scraper.scrape_sources(
    sources=enabled_sources,
    query=query,
    max_articles_per_source=5
)

# Process results
for article in articles:
    print(f"Title: {{article.title}}")
    print(f"Source: {{article.source_name}}")
    print(f"URL: {{article.url}}")
    print(f"Content: {{article.content[:200]}}...")
```

Note: Actual scraping of news sites is disabled in this demo to avoid
overwhelming servers. Enable it by uncommenting the code below and
ensuring you have internet connectivity.
""")
    
    # Uncomment to test real scraping (requires internet)
    """
    print(f"\nüöÄ Attempting real scraping (1 article from first source)...")
    if enabled_sources:
        try:
            articles = await scraper.scrape_search_results(
                enabled_sources[0],
                query="technology",
                max_articles=1
            )
            
            if articles:
                article = articles[0]
                print(f"\n‚úÖ Scraped 1 article:")
                print(f"   Source: {article.source_name}")
                print(f"   Title: {article.title}")
                print(f"   URL: {article.url}")
                print(f"   Content: {len(article.content)} chars")
        except Exception as e:
            print(f"‚ùå Scraping error: {e}")
    """
    
    print(f"\n" + "=" * 70)
    print("‚úÖ DEMO COMPLETE")
    print("=" * 70)
    print("\nKey Features Demonstrated:")
    print("  ‚úì Async URL fetching with httpx")
    print("  ‚úì Automatic retry with exponential backoff")
    print("  ‚úì Per-domain rate limiting")
    print("  ‚úì Content extraction with BeautifulSoup")
    print("  ‚úì Generic and selector-based extraction")
    print("  ‚úì Comprehensive error handling")
    print("  ‚úì Logging for all operations")
    print("\nThe scraping engine is ready for production use!")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(demo_scraping())
